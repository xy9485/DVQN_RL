ALE/Breakout-v5:
  env_type: Atari
  total_timesteps: 300000
  init_steps: 5000
  input_format: full_img
  grd_hidden_channels:
    - 32
    - 64
    - 64
  grd_embedding_dim: 64
  encoder_detach: False
  use_shaping: True
  # cluster_embedding_dim: None
  num_vq_embeddings: 128
  dim_vq_embeddings: 64
  mlp_hidden_dim_abs: 64
  mlp_hidden_dim_grd: 64
  # lr_ground_Q: 0.0002
  # lr_abstract_V: 0.1
  lr_ground_Q: 0.0002
  lr_abstract_V: 0.0005
  lr_vq: 0.0005
  # lr_encoder: 0.0005
  # lr_decoder: 0.0005
  batch_size: 256
  size_replay_memory: 50000
  gamma: 0.99
  abstract_gamma: 0.99
  omega: 10
  ground_Q_encoder_tau: 0.05
  ground_Q_critic_tau: 0.05
  abstract_tau: 1
  # encoder_tau: 0.2
  exploration_fraction: 1
  exploration_initial_eps: 0.1
  exploration_final_eps: 0.01
  save_model_every: 500000.0
  # tcurl_learn_every: 4
  ground_learn_every: 4
  ground_sync_every: 64
  ground_gradient_steps: 1
  abstract_learn_every: 0
  abstract_sync_every: 8
  abstract_gradient_steps: 0
  clip_grad: True
  clip_reward: True
  encoded_detach: True
  # wandb_group: AE_KMeans_DQN
  # wandb_group: AE_DQN
  wandb_group: e2e_100k_buf50k_detach
  # wandb_group: shp_100k_lrabs0.1_tau0.1_aevery4_rrward_mod
  wandb_notes: nothing
  # wandb_tags: ["shp", "omg1", "tbl", "16x16", "(1,1)"]

  # validate_every: 1000
  reset_training_info_every: 1000
  save_recon_every: 1000
  # buffer_recent_states_every: 1000

Atari:
  env_type: Atari
  domain_name: "ALE/Asterix-v5"
  #Asterix-v5; Freeway-v5; MsPacman-v5; Seaquest-v5; Breakout-v5; Boxing-v5; Riverraid-v5; Pong-v5; Frostbite-v5
  total_timesteps: 100000
  init_steps: 10000
  input_format: full_img
  grd_hidden_channels:
    - 32
    - 64
    - 64
  grd_encoder_linear_dims: null
  abs_encoder_linear_dims: null
  encoded_detach4abs: False
  use_shaping: True
  # cluster_embedding_dim: None
  num_vq_embeddings: 50
  dim_vq_embeddings: 512
  vq_softmin_beta: 10
  mlp_hidden_dim_abs: 256
  mlp_hidden_dim_grd: 256
  # lr_ground_Q: 0.0002
  # lr_abstract_V: 0.1
  lr_ground_Q: 0.0001
  lr_abstract_V: 0.0001
  lr_vq: 0.0001
  lr_curl: 0.0001
  safe_ratio: lin_1.0
  # lr_encoder: 0.0005
  # lr_decoder: 0.0005
  batch_size: 128
  batch_size_repre: 128
  size_replay_memory: 100000
  gamma: 0.99
  abstract_gamma: 0.99
  omega: 1
  ground_Q_encoder_tau: 1
  ground_Q_critic_tau: 1
  abstract_V_encoder_tau: 1
  abstract_V_critic_tau: 1
  curl_tau: 1
  # encoder_tau: 0.2
  exploration_fraction: 0.9
  exploration_initial_eps: 0.2
  exploration_final_eps: 0.01
  save_model_every: 500000.0
  # tcurl_learn_every: 4
  ground_learn_every: 1
  ground_sync_every: 2000
  # ground_gradient_steps: 1
  abstract_learn_every: 1
  abstract_sync_every: 2000
  # abstract_gradient_steps: 1
  curl_vq_learn_every: 1
  curl_vq_gradient_steps: 1
  curl_vq_sync_every: 2000
  clip_grad: True
  clip_reward: True
  # wandb_group: AE_KMeans_DQN
  # wandb_group: AE_DQN
  wandb_group: 10w_B10w_safe0-1
  # wandb_group: ALE/Asterix-v5_10w_B10w_close+ct21
  # wandb_group: shp_100k_lrabs0.1_tau0.1_aevery4_rrward_mod
  wandb_notes: HDQN;no vq; with contrastive learning; n_step_return=5
  # wandb_tags: ["shp", "omg1", "tbl", "16x16", "(1,1)"]

  # validate_every: 1000
  reset_training_info_every: 1000
  save_recon_every: 1000
  # buffer_recent_states_every: 1000
