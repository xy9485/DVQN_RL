ALE/Breakout-v5:
  env_type: Atari
  total_timesteps: 300000
  init_steps: 5000
  input_format: full_img
  grd_hidden_channels:
    - 32
    - 64
    - 64
  grd_embedding_dim: 64
  encoder_detach: False
  use_shaping: True
  # cluster_embedding_dim: None
  num_vq_embeddings: 128
  dim_vq_embeddings: 64
  mlp_hidden_dim_abs: 64
  mlp_hidden_dim_grd: 64
  # lr_ground_Q: 0.0002
  # lr_abstract_V: 0.1
  lr_ground_Q: 0.0002
  lr_abstract_V: 0.0005
  lr_vq: 0.0005
  # lr_encoder: 0.0005
  # lr_decoder: 0.0005
  batch_size: 256
  size_replay_memory: 50000
  gamma: 0.99
  abstract_gamma: 0.99
  omega: 10
  ground_Q_encoder_tau: 0.05
  ground_Q_critic_tau: 0.05
  abstract_tau: 1
  # encoder_tau: 0.2
  exploration_fraction: 1
  exploration_initial_eps: 0.1
  exploration_final_eps: 0.01
  save_model_every: 500000.0
  # tcurl_learn_every: 4
  ground_learn_every: 4
  ground_sync_every: 64
  ground_gradient_steps: 1
  abstract_learn_every: 0
  abstract_sync_every: 8
  abstract_gradient_steps: 0
  clip_grad: True
  clip_reward: True
  encoded_detach: True
  # wandb_group: AE_KMeans_DQN
  # wandb_group: AE_DQN
  wandb_group: e2e_100k_buf50k_detach
  # wandb_group: shp_100k_lrabs0.1_tau0.1_aevery4_rrward_mod
  wandb_notes: nothing
  # wandb_tags: ["shp", "omg1", "tbl", "16x16", "(1,1)"]

  # validate_every: 1000
  reset_training_info_every: 1000
  save_recon_every: 1000
  # buffer_recent_states_every: 1000

Atari:
  env_type: Atari
  total_timesteps: 300000
  init_steps: 1600
  input_format: full_img
  grd_hidden_channels:
    - 32
    - 64
    - 64
  use_shaping: True
  # cluster_embedding_dim: None
  encoded_detach4abs: False
  grd_embedding_dim: 128
  num_vq_embeddings: 256
  dim_vq_embeddings: 128
  vq_softmin_beta: 1
  mlp_hidden_dim_abs: 256
  mlp_hidden_dim_grd: 256
  # lr_ground_Q: 0.0002
  # lr_abstract_V: 0.1
  lr_ground_Q: 0.0001
  lr_abstract_V: 0.0001
  lr_vq: 0.0001
  # lr_encoder: 0.0005
  # lr_decoder: 0.0005
  batch_size: 32
  size_replay_memory: 100000
  gamma: 0.99
  abstract_gamma: 0.99
  omega: 10
  ground_Q_encoder_tau: 1.0
  ground_Q_critic_tau: 1.0
  abstract_tau: 1
  # encoder_tau: 0.2
  exploration_fraction: 0.9
  exploration_initial_eps: 0.1
  exploration_final_eps: 0.01
  save_model_every: 500000.0
  # tcurl_learn_every: 4
  ground_learn_every: 1
  ground_sync_every: 1000
  ground_gradient_steps: 1
  abstract_learn_every: 0
  abstract_sync_every: 8
  abstract_gradient_steps: 0
  clip_grad: True
  clip_reward: True
  # wandb_group: AE_KMeans_DQN
  # wandb_group: AE_DQN
  wandb_group: Alien_e2e_300k_buf100k_vqbeta3_pure
  # wandb_group: shp_100k_lrabs0.1_tau0.1_aevery4_rrward_mod
  wandb_notes: nothing
  # wandb_tags: ["shp", "omg1", "tbl", "16x16", "(1,1)"]

  # validate_every: 1000
  reset_training_info_every: 1000
  save_recon_every: 1000
  # buffer_recent_states_every: 1000
