MinAtar:
  env_type: MinAtar
  total_timesteps: 1000000
  init_steps: 5000
  input_format: full_img_small
  grd_hidden_channels: 16 # int|list
  grd_encoder_linear_dims: null # int|list|None
  encoded_detach4abs: False
  use_shaping: True
  # cluster_embedding_dim: None
  num_vq_embeddings: 128
  dim_vq_embeddings: 32
  vq_softmin_beta: 10
  mlp_hidden_dim_abs: 128
  mlp_hidden_dim_grd: 128
  # lr_ground_Q: 0.0002
  # lr_abstract_V: 0.1
  lr_ground_Q: 0.00025
  lr_abstract_V: 0.00025
  lr_vq: 0.00025
  lr_curl: 0.00025
  # lr_encoder: 0.0005
  # lr_decoder: 0.0005
  batch_size: 128
  batch_size_repre: 128
  size_replay_memory: 200000
  gamma: 0.99
  abstract_gamma: 0.99
  omega: 1
  ground_Q_encoder_tau: 0.01
  ground_Q_critic_tau: 0.01
  abstract_tau: 0.01
  curl_tau: 0.03
  # encoder_tau: 0.2

  exploration_fraction: 0.1
  exploration_initial_eps: 1.0
  exploration_final_eps: 0.1
  save_model_every: 500000.0

  ground_learn_every: 1
  ground_sync_every: 2
  # ground_gradient_steps: 1
  abstract_learn_every: 1
  abstract_sync_every: 2
  # abstract_gradient_steps: 1
  curl_vq_learn_every: 1
  curl_vq_gradient_steps: 1
  curl_vq_sync_every: 2

  clip_grad: True
  clip_reward: True
  # wandb_group: AE_KMeans_DQN
  # wandb_group: AE_DQN
  wandb_group: Seaquest-v0_1M_B20w_ddqn_close_test4#
  # wandb_group: shp_100k_lrabs0.1_tau0.1_aevery4_rrward_mod
  # wandb_notes: total_loss = 3.0 * ground_td_error - entrophy_vq + codebook_diversity + commit_error_abs2grd(abs_v, grd_q_reduction)
  wandb_notes: with contrastive learning, wandb log every 1 episode

  # wandb_tags: ["shp", "omg1", "tbl", "16x16", "(1,1)"]

  # validate_every: 1000
  reset_training_info_every: 1000
  save_recon_every: 1000
  # buffer_recent_states_every: 1000
